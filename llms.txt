# AI SDK Documentation Reference

This file provides AI assistants (Cursor, Windsurf, Copilot, Claude, etc.) with comprehensive documentation about the AI SDK and this project.

## AI SDK Documentation

Access the complete AI SDK documentation at:
https://ai-sdk.dev/llms.txt

You can also browse the full documentation at:
https://sdk.vercel.ai/docs

### Key Resources

- **Getting Started**: https://sdk.vercel.ai/docs/getting-started
- **Core Concepts**: https://sdk.vercel.ai/docs/concepts
- **Providers**: https://sdk.vercel.ai/docs/providers
- **Tools**: https://sdk.vercel.ai/docs/concepts/tools
- **Streaming**: https://sdk.vercel.ai/docs/concepts/streaming

## This Project - AI Elements Chat

### Project Overview

This is a full-featured AI chatbot built with:
- Next.js 14 App Router
- AI SDK by Vercel
- Multiple AI providers (Google Gemini, Hugging Face, OpenAI, Anthropic)
- Authentication (Auth.js)
- Data persistence (Neon Postgres, Drizzle ORM)
- File storage (Vercel Blob)
- Tool integration
- shadcn/ui components
- Tailwind CSS + Geist typography

### File Structure

```
app/
├── api/
│   ├── auth/          # Auth.js authentication routes
│   ├── chat/          # AI chat API with streaming
│   └── tools/         # Tool endpoints (weather, etc.)
├── globals.css        # Tailwind + Geist typography
├── layout.tsx         # Root layout with auth
└── page.tsx           # Main chat interface

lib/
├── db/
│   ├── schema.ts      # Database schema (users, sessions, messages, files)
│   └── index.ts       # Database connection
├── agent-logic.ts     # Auto-learning AI agent logic
├── auth.ts            # NextAuth configuration
├── providers.ts       # AI provider configurations
├── storage.ts         # Vercel Blob storage utilities
└── tools.ts           # AI SDK tools (weather, calculator, search)
```

### AI Providers

#### Google Gemini (Primary)
- Provider: `@ai-sdk/google`
- Models: `gemini-2.5-pro`, `gemini-2.5-flash`, `gemini-2.5-flash-lite`
- Features: Fast, low-latency, multi-modal

#### Hugging Face
- Provider: OpenAI-compatible endpoint
- Models: `deepseek-ai/DeepSeek-V3-0324`, `meta-llama/Llama-3.2-3B-Instruct`
- Features: Thousands of models, free tier

#### OpenAI
- Provider: `@ai-sdk/openai`
- Models: `gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`

#### Anthropic
- Provider: `@ai-sdk/anthropic`
- Models: `claude-3-5-sonnet`, `claude-3-opus`, `claude-3-haiku`
- Features: Long context (200K tokens)

### Tool Integration

#### Built-in Tools
1. **weather** - Get weather for a location
2. **calculate** - Perform mathematical calculations
3. **search** - Search the web

#### Tool Usage
Tools are integrated in `app/api/chat/route.ts`:
```typescript
import { tools } from '@/lib/tools';

streamTextOptions = {
  model: google('gemini-2.5-flash'),
  messages,
  tools,              // Pass tools here
  maxSteps: 5,        // Max tool call chains
};
```

### Authentication

- Using NextAuth (Auth.js)
- Providers: GitHub, Google
- Session management with database
- User profile storage

### Data Persistence

#### Database Schema (Drizzle ORM + Neon Postgres)
- **users** - User accounts and profiles
- **sessions** - Auth sessions
- **accounts** - OAuth connections  
- **chat_sessions** - Conversation metadata
- **messages** - Individual messages
- **files** - Uploaded files metadata

#### Initialization
```bash
npx drizzle-kit push      # Create tables
npx prisma generate       # Generate Prisma client
```

### File Storage

Using Vercel Blob for:
- User uploads
- AI-generated assets
- Chat attachments

### Radix UI & Styling

This project uses Radix UI primitives (via shadcn/ui) for unstyled, accessible components.

### Radix Primitives
- Completely unstyled - you have full control
- Accessible by default (keyboard, ARIA, screen readers)
- Works with any styling solution (Tailwind, CSS, styled-components)
- Production-ready and battle-tested

### Styling Approach
This project uses:
- **Radix Primitives** - Base functionality
- **shadcn/ui** - Radix + Tailwind CSS
- **Tailwind CSS** - Utility-first CSS
- **Complete Control** - Override any styling

See `lib/radix-guide.md` for custom Radix components.

## Key Dependencies

```json
{
  "@ai-sdk/google": "^1.0.17",
  "@ai-sdk/openai": "^2.0.53",
  "@ai-sdk/anthropic": "^0.0.51",
  "@ai-sdk/react": "^0.0.33",
  "ai": "^3.0.14",
  "next": "^14.2.13",
  "next-auth": "beta",
  "zod": "^3.25.76",
  "@vercel/blob": "latest",
  "@neondatabase/serverless": "latest",
  "drizzle-orm": "latest",
  "@auth/drizzle-adapter": "latest"
}
```

### Marketing Components

This project includes Tailwind UI-inspired marketing components:
- `components/marketing/hero-section.tsx` - Landing page hero
- `components/marketing/feature-grid.tsx` - Feature showcase
- `components/marketing/newsletter-signup.tsx` - Email collection
- `components/marketing/pricing-table.tsx` - Pricing plans

All built with Radix UI, Headless UI, and Tailwind CSS.

### Radix UI Primitives

This project includes Radix UI primitives for complete styling control:
- **@radix-ui/react-dialog** - Modal dialogs
- **@radix-ui/react-tabs** - Tab navigation
- **@radix-ui/react-dropdown-menu** - Menus
- **@radix-ui/react-select** - Select components
- And more...

See `components/custom-dialog.tsx` and `components/custom-tabs.tsx` for examples.

## Usage Examples

### Basic Chat
```typescript
import { useChat } from '@ai-sdk/react';

const { messages, input, handleSubmit } = useChat({
  api: '/api/chat',
  body: { provider: 'google' }
});
```

### With Tools
```typescript
import { tools } from '@/lib/tools';
import { streamText } from 'ai';

const result = await streamText({
  model: google('gemini-2.5-flash'),
  prompt: 'What is the weather in San Francisco?',
  tools,
});
```

### Provider Switching
```typescript
const [provider, setProvider] = useState('google');

const { messages } = useChat({
  body: { provider }
});
```

## Auto-Learning Agent

The app includes intelligent agent logic in `lib/agent-logic.ts`:
- Task-aware model selection
- Cost optimization
- Latency optimization
- User preference learning

## Environment Variables

Required:
```env
# At least one AI provider
GOOGLE_GENERATIVE_AI_API_KEY=...
# OR
HF_TOKEN=...
# OR
OPENAI_API_KEY=...
```

Optional (for full features):
```env
DATABASE_URL=...                    # Neon Postgres
GITHUB_CLIENT_ID=...               # Auth.js
GITHUB_CLIENT_SECRET=...
GOOGLE_CLIENT_ID=...
GOOGLE_CLIENT_SECRET=...
NEXTAUTH_SECRET=...
BLOB_READ_WRITE_TOKEN=...           # Vercel Blob
```

## Documentation Files

- **GETTING_STARTED.md** - Quick start guide
- **INSTALLATION.md** - Installation guide
- **FULL_FEATURES_SETUP.md** - Complete feature setup
- **HUGGING_FACE_GUIDE.md** - HF integration details
- **LOCAL_AI.md** - Local inference setup
- **PROVIDERS.md** - AI provider comparisons
- **.cursorrules** - AI agent configuration
- **mcp-config.json** - MCP server configuration

## Development Commands

```bash
npm run dev          # Start dev server
npm run build        # Build for production
npm run lint         # Run linter
npx prisma studio    # Database GUI (if using Prisma)
npx drizzle-kit push # Push schema changes
```

## R2AG Framework

This project includes a **Reasoning-Augmented Retrieval-Augmented Generation** framework:

### Features
- Document chunking and embedding
- Cosine similarity retrieval
- Reasoning layer for context analysis
- Complete R2AG pipeline
- Agent integration

### Usage
```typescript
import { R2AGFramework } from '@/lib/rag-engine';

const rag = new R2AGFramework();
await rag.addDocuments(texts, { source: 'docs' });
const result = await rag.processQuery('your question');
```

### API Endpoints
- `POST /api/rag` - Query with RAG
- `PUT /api/rag` - Add documents
- `GET /api/rag` - Get statistics

### Agent with RAG
```typescript
import { createR2AGAgent } from '@/lib/rag-agent';

const agent = createR2AGAgent(rag);
const result = await agent.generate({ prompt: 'question' });
```

## Common Patterns

### Streaming Response
```typescript
const result = await streamText({
  model: google('gemini-2.5-flash'),
  messages,
});
return result.toDataStreamResponse();
```

### Tool Definition
```typescript
export const myTool = tool({
  description: 'Tool description',
  parameters: z.object({
    param: z.string().describe('Param description'),
  }),
  execute: async ({ param }) => {
    return { result: 'data' };
  },
});
```

### Adding Provider
```typescript
// In app/api/chat/route.ts
case 'my-provider':
  streamTextOptions = {
    model: myProvider(model || 'default'),
    messages,
    tools,
  };
  break;
```

## Quick Links

- Project README: [README.md](README.md)
- AI SDK Docs: https://sdk.vercel.ai/docs
- AI SDK llms.txt: https://ai-sdk.dev/llms.txt
- Next.js Docs: https://nextjs.org/docs
- Drizzle ORM: https://orm.drizzle.team
- NextAuth: https://authjs.dev
- Vercel Blob: https://vercel.com/docs/storage
- Neon: https://neon.tech/docs

## Notes for AI Assistants

When working with this codebase:
1. Use `lib/tools.ts` for tool definitions
2. Extend `lib/agent-logic.ts` for smarter model selection
3. Add providers in `app/api/chat/route.ts` switch statement
4. Database schema is in `lib/db/schema.ts`
5. Auth config is in `lib/auth.ts`
6. All environment variables are documented

For AI SDK specific questions, reference: https://ai-sdk.dev/llms.txt

