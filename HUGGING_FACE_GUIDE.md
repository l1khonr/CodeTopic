# Hugging Face Integration Guide

Your AI Elements Chat app is fully integrated with Hugging Face Inference API, giving you access to **thousands of cutting-edge AI models**!

## üöÄ What You Can Do

### 1. **Text Generation**
Generate human-like text with state-of-the-art models:
- **DeepSeek V3** - Advanced reasoning and coding
- **Llama 3.2** - Latest Meta AI model
- **Qwen** - Powerful multilingual models
- **Gemma** - Google's open-source models

### 2. **Multi-Language Support**
Access models trained on multiple languages for global applications:
- English, Spanish, French, Chinese, Japanese, and more
- Native multilingual understanding

### 3. **Specialized Models**
For specific tasks:
- **Sentiment Analysis** - Detect emotions in text
- **Text Classification** - Categorize documents
- **Summarization** - Extract key points from long texts
- **Translation** - Translate between languages

## üìã Available Models

Your app supports these Hugging Face models by default:

### Recommended Models

| Model | Size | Best For | Context |
|-------|------|----------|---------|
| **deepseek-ai/DeepSeek-V3-0324** | 16B | Advanced reasoning, coding, math | 128K |
| **meta-llama/Llama-3.3-70B-Instruct** | 70B | Complex tasks, analysis | 128K |
| **meta-llama/Llama-4-Scout-17B-16E-Instruct** | 17B | Multi-agent reasoning | 128K |
| **Qwen/Qwen3-235B-A22B-Instruct** | 235B | Maximum capability | 128K |

### Quick Models

| Model | Size | Best For |
|-------|------|----------|
| **meta-llama/Llama-3.2-3B-Instruct** | 3B | Fast responses |
| **Qwen/Qwen2.5-3B-Instruct** | 3B | Multilingual, compact |

## üéØ How to Use

### In the Chat Interface

1. Set your HF token in `.env.local`:
   ```env
   HF_TOKEN=your_huggingface_token_here
   ```

2. Select "Hugging Face" as your provider

3. Start chatting! The app will use your Hugging Face token to access models.

### Model Selection

You can specify different models. The app defaults to `meta-llama/Llama-3.2-3B-Instruct` but you can use any available model:

**Lightweight & Fast:**
- `meta-llama/Llama-3.2-3B-Instruct`
- `google/gemma-3-27b-it`
- `Qwen/Qwen2.5-3B-Instruct`

**Powerful & Capable:**
- `deepseek-ai/DeepSeek-V3-0324`
- `meta-llama/Llama-3.3-70B-Instruct`
- `Qwen/Qwen3-235B-A22B-Instruct`

## üí° Example Use Cases

### 1. **Code Generation**
```
Model: deepseek-ai/DeepSeek-V3-0324
Prompt: "Write a Python function to calculate fibonacci numbers with memoization"
```

### 2. **Multi-language Support**
```
Model: Qwen/Qwen2.5-3B-Instruct
Prompt: "Translate 'Hello, how are you?' to Spanish, French, and Japanese"
```

### 3. **Creative Writing**
```
Model: meta-llama/Llama-3.1-8B-Instruct
Prompt: "Write a short story about a robot discovering emotions"
```

### 4. **Technical Documentation**
```
Model: deepseek-ai/DeepSeek-V3-0324
Prompt: "Explain React hooks with code examples"
```

## üîç Exploring More Models

Visit [Hugging Face Inference Models](https://huggingface.co/inference/models) to discover:

- Latest trending models
- Model capabilities (text generation, vision, audio)
- Performance metrics
- Pricing information
- Community feedback

You can use any model by specifying its ID in your API calls.

## üìä Hugging Face Inference Providers

Your integration uses the **Hugging Face Router** which provides access to 15+ inference partners:

- ‚úÖ **Groq** - Ultra-fast inference
- ‚úÖ **Together AI** - High-throughput models
- ‚úÖ **Replicate** - Easy model hosting
- ‚úÖ **Fireworks** - Production-ready models
- ‚úÖ **Cerebras** - Enterprise AI
- ‚úÖ **SambaNova** - Large-scale inference
- ‚úÖ **Cohere** - NLP-focused models
- ...and many more!

No infrastructure to manage - Hugging Face handles it all!

## üé® Free Tier

Hugging Face offers a **free tier** that includes:
- Access to hundreds of models
- Generous rate limits for development
- No credit card required to start
- Easy upgrades for production use

## üîê Security

Your HF_TOKEN in `.env.local` is secure:
- Never commit it to git (it's in `.gitignore`)
- Used only server-side in API routes
- Automatically handled by Edge Runtime

## üöÄ Performance Tips

1. **Use smaller models** for faster responses (3B-7B parameters)
2. **Cache frequently used prompts** to save costs
3. **Choose the right model** for your task (coding vs creative writing)
4. **Monitor usage** in your [Hugging Face dashboard](https://huggingface.co/settings/billing)

## üìö Additional Resources

- [Hugging Face Models](https://huggingface.co/models) - Browse all models
- [Inference API Docs](https://huggingface.co/docs/api-inference) - API reference
- [Model Cards](https://huggingface.co/docs/hub/models-cards) - Understand model capabilities
- [Community Forum](https://discuss.huggingface.co) - Get help from experts

## üéâ Getting Started

Your Hugging Face integration is already configured! Just:

1. ‚úÖ Set your token in `.env.local`
2. Select "Hugging Face" in the provider dropdown
3. Start chatting with any of thousands of models!

Happy building! üöÄ

